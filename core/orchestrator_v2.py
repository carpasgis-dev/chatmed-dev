"""
ChatMed Flexible v2.0 - Orquestador Inteligente Ultra-Optimizado
=================================================================

Orquestador de nueva generaci√≥n con:
- ‚ö° Cache inteligente multinivel
- üöÄ Clasificaci√≥n ultra-r√°pida con fallback optimizado
- üîÑ Arquitectura flexible y modular
- üìä Monitoreo de rendimiento en tiempo real
- üéØ Rutas optimizadas basadas en patrones de uso

Autor: Carmen Pascual
Versi√≥n: 2.0 - Flexible
"""

import logging
import os
import asyncio
import warnings
import time
from typing import Dict, Any, Tuple, Optional, List, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import json
from pathlib import Path
import re
from .memory_manager import load_memory, save_memory, UserMemory

# Corregido: Importar SecretStr desde la ruta correcta para evitar conflictos de pydantic.
from pydantic.v1.types import SecretStr

# NUEVO: Importar tipos de mensajes de OpenAI para compatibilidad
from openai.types.chat import ChatCompletionSystemMessageParam, ChatCompletionUserMessageParam

# --- Dependencias de LangChain ---
try:
    from langchain_openai import ChatOpenAI
    from langchain.schema import HumanMessage, SystemMessage
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("‚ö†Ô∏è LangChain no disponible. Funcionar√° con clasificaci√≥n local.")

# --- Agentes M√©dicos Especializados ---
# Intentar importar agentes v2 disponibles
AGENTS_V2_AVAILABLE = False
try:
    # Imports absolutos para evitar problemas con imports relativos
    import sys
    import os
    current_dir = os.path.dirname(__file__)
    agents_dir = os.path.join(current_dir, '..', 'agents')
    sys.path.insert(0, agents_dir)
    
    # Importar agentes usando imports absolutos
    from agents.biochat_agent import BioChatAgent  # type: ignore
    from agents.greeting_agent import IntelligentGreetingAgent  # type: ignore
    from agents.pubmed_query_generator import PubMedQueryGenerator  # type: ignore
    from agents.sql_agent_flexible_enhanced import SQLAgentIntelligentEnhanced  # type: ignore  # SQL Agent original que funciona
    from agents.medgemma_clinical_agent import MedGemmaClinicalAgent  # type: ignore  # NUEVO: Agente MedGemma para an√°lisis cl√≠nico
    from agents.fhir_agent_complete import FHIRMedicalAgent  # type: ignore  # Agente FHIR completo
    from agents.fhir_persistence_agent_old import FHIRPersistenceAgent  # type: ignore  # Agente de persistencia FHIR (versi√≥n antigua)
    AGENTS_V2_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Error importando agentes v2: {e}")
    AGENTS_V2_AVAILABLE = False

# Fallback a agentes v1 - DESHABILITADO para evitar conflictos
AGENTS_V1_AVAILABLE = False

# --- Configuraci√≥n General ---
logging.basicConfig(level=logging.WARNING, format='%(asctime)s [%(levelname)s] %(message)s')
warnings.filterwarnings("ignore")
logger = logging.getLogger("ChatMedFlexibleV2")

class QueryType(Enum):
    """Tipos de consulta para clasificaci√≥n r√°pida"""
    GREETING = "greeting"
    SQL_QUERY = "sql"
    FHIR_PROCESSING = "fhir"
    BIOCHAT_SEARCH = "biochat"
    CLINICAL_ANALYSIS = "clinical_analysis"  # NUEVO: An√°lisis cl√≠nico con MedGemma
    FOLLOW_UP = "follow_up"
    CLINICAL_NOTE = "clinical_note"
    UNKNOWN = "unknown"

@dataclass
class CacheEntry:
    """Entrada de cache con metadatos"""
    query_hash: str
    agent_type: str
    confidence: float
    result: Any
    timestamp: datetime
    hit_count: int = 0
    avg_response_time: float = 0.0
    
    def is_expired(self, ttl_minutes: int = 60) -> bool:
        """Verifica si la entrada ha expirado"""
        return datetime.now() - self.timestamp > timedelta(minutes=ttl_minutes)

@dataclass
class PerformanceMetrics:
    """M√©tricas de rendimiento del orquestador"""
    total_queries: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    avg_response_time: float = 0.0
    agent_usage: Dict[str, int] = field(default_factory=dict)
    classification_time: float = 0.0
    execution_time: float = 0.0
    llm_calls: int = 0
    local_classifications: int = 0
    
    @property
    def cache_hit_rate(self) -> float:
        """Tasa de acierto del cache"""
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0

class Colors:
    """Colores ANSI para la consola"""
    GREEN = '\033[92m'
    BLUE = '\033[94m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    PURPLE = '\033[95m'
    CYAN = '\033[96m'
    BOLD = '\033[1m'
    END = '\033[0m'

class FlexibleOrchestrator:
    """
    üéØ Orquestador Flexible v2.0 - Ultra-Optimizado
    
    Caracter√≠sticas principales:
    - Cache inteligente multinivel con TTL
    - Clasificaci√≥n ultra-r√°pida con patrones optimizados
    - Arquitectura modular y flexible
    - Monitoreo de rendimiento en tiempo real
    - Rutas optimizadas basadas en uso
    - Fallback inteligente sin dependencias externas
    """
    
    @staticmethod
    def _call_openai_native(client, messages, temperature=0.1, max_tokens=4000):
        """
        Funci√≥n de compatibilidad para llamar a OpenAI nativo con streaming y logging.
        Ahora es un m√©todo est√°tico para ser heredado correctamente.
        """
        try:
            # Logging m√°s conciso
            from openai import OpenAI
            native_client = OpenAI()

            if isinstance(messages, list):
                openai_messages: List[Union[ChatCompletionSystemMessageParam, ChatCompletionUserMessageParam]] = []
                for msg in messages:
                    role = "user"
                    content = ""
                    if hasattr(msg, 'content'):
                        content = str(msg.content)
                        if isinstance(msg, SystemMessage):
                            role = "system"
                    elif isinstance(msg, dict) and "role" in msg and "content" in msg:
                        role = str(msg["role"])
                        content = str(msg["content"])
                    else:
                        content = str(msg)
                    
                    if role == "system":
                        openai_messages.append(ChatCompletionSystemMessageParam(role="system", content=content))
                    else:
                        openai_messages.append(ChatCompletionUserMessageParam(role="user", content=content))
            else:
                content = messages.content if hasattr(messages, 'content') else str(messages)
                openai_messages = [ChatCompletionUserMessageParam(role="user", content=str(content))]

            # Sin streaming para el orquestador
            response = native_client.chat.completions.create(
                model='gpt-4o',
                messages=openai_messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            content = response.choices[0].message.content or ""

            if not content.strip():
                content = '{"success": false, "message": "Error: Respuesta vac√≠a del LLM"}'

            return content

        except Exception as e:
            error_msg = f"Error en llamada OpenAI del Orquestador: {str(e)}"
            logger.error(f"Error en _call_openai_native (Orquestador): {e}", exc_info=True)
            return '{"success": false, "message": "Error cr√≠tico en la llamada al LLM."}'
    
    def __init__(self, 
                 config_path: Optional[str] = None,
                 enable_cache: bool = True,
                 enable_performance_monitoring: bool = True,
                 cache_ttl_minutes: int = 60):
        """
        Inicializa el Orquestador Flexible v2.0
        """
        logger.info("üöÄ Inicializando ChatMed Flexible Orchestrator v2.0...")
        
        # Configuraci√≥n b√°sica
        self.config_path = config_path
        self.enable_cache = enable_cache
        self.enable_performance_monitoring = enable_performance_monitoring
        self.cache_ttl_minutes = cache_ttl_minutes
        
        # Cargar configuraci√≥n flexible
        self._load_flexible_config()
        
        # Inicializar LLM para clasificaci√≥n inteligente
        self.llm_classifier = None
        self._initialize_llm_classifier()
        
        # Inicializar agentes v2
        self.agents = {}
        self._initialize_agents_v2()
        
        # Cache y m√©tricas
        self.query_cache = {} if enable_cache else None
        self.classification_cache = {} if enable_cache else None
        self.metrics = PerformanceMetrics() if enable_performance_monitoring else None
        
        # Configuraci√≥n de clasificaci√≥n inteligente
        self.use_intelligent_classification = True
        self.classification_confidence_threshold = 0.6
        
        # Memoria de conversaci√≥n
        self.conversation_history = []
        self.current_user_id = "default"
        
        # Cargar memoria de usuario
        self.user_memories = load_memory()
        
        logger.info("‚úÖ Orquestador Flexible v2.0 inicializado")
        self._log_initialization_summary()
    
    def _load_flexible_config(self):
        try:
            config_paths = [
                "chatmed_v2_flexible/config/orchestrator_config.yaml",
                "../config/orchestrator_config.yaml"
            ]
            
            # Solo agregar config_path si no es None
            if self.config_path is not None:
                config_paths.insert(0, self.config_path)
            
            for path in config_paths:
                if path is not None and os.path.exists(path):
                    self.flexible_config = {
                        'cache_enabled': True,
                        'llm_model': 'gpt-4o',
                        'fallback_confidence_threshold': 0.6,
                        'max_conversation_history': 20,
                        'enable_smart_routing': True,
                        'enable_learning': True
                    }
                    logger.info(f"‚úÖ Configuraci√≥n flexible cargada desde: {path}")
                    return
            
            self.flexible_config = {
                'cache_enabled': True,
                'llm_model': 'gpt-4o',
                'fallback_confidence_threshold': 0.6,
                'max_conversation_history': 20,
                'enable_smart_routing': True,
                'enable_learning': True
            }
            logger.info("‚ö†Ô∏è Usando configuraci√≥n por defecto")
            
        except Exception as e:
            logger.error(f"‚ùå Error cargando configuraci√≥n: {e}")
            self.flexible_config = {}
    
    def _initialize_llm_classifier(self):
        if not LANGCHAIN_AVAILABLE:
            logger.warning("‚ö†Ô∏è LangChain no disponible. Usando clasificaci√≥n local.")
            return
            
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                logger.warning("‚ö†Ô∏è OPENAI_API_KEY no encontrada. Clasificaci√≥n local activada.")
                return
            
            model = self.flexible_config.get('llm_model', 'gpt-4o')
            self.llm_classifier = ChatOpenAI()
            logger.info(f"‚úÖ Clasificador LLM ({model}) inicializado.")
            
        except Exception as e:
            logger.error(f"‚ùå Error inicializando LLM: {e}")
    
    def _initialize_agents_v2(self):
        logger.info("üîß Inicializando agentes v2.0...")
        
        db_path = self._find_database()
        if not db_path:
            logger.error("‚ùå Base de datos no encontrada")
            return
        
        llm_instance = self.llm_classifier
        
        if not llm_instance:
            logger.warning("‚ö†Ô∏è No se puede inicializar agentes que requieren LLM. ¬øFalta la API Key de OpenAI?")
            logger.warning("‚ö†Ô∏è Algunas funciones estar√°n limitadas.")

        try:
            if AGENTS_V2_AVAILABLE:
                agents_initialized = []
                logger.info(f"[DEBUG] AGENTS_V2_AVAILABLE: {AGENTS_V2_AVAILABLE}")
                # Crear cliente nativo de OpenAI para agentes que lo requieren
                try:
                    from openai import OpenAI
                    openai_client = OpenAI()
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è No se pudo crear cliente OpenAI nativo: {e}")
                    openai_client = None
                
                if llm_instance:
                    try:
                        self.agents['greeting'] = IntelligentGreetingAgent(llm=llm_instance)  # type: ignore
                        agents_initialized.append('greeting')
                        logger.info("[DEBUG] Agente 'greeting' inicializado")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error inicializando agente de saludos: {e}")
                
                if llm_instance:
                    try:
                        self.agents['biochat'] = BioChatAgent(llm=llm_instance, medgemma_agent=None)  # type: ignore
                        agents_initialized.append('biochat')
                        logger.info("[DEBUG] Agente 'biochat' inicializado")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error inicializando agente BioChat: {e}")
                
                if openai_client:
                    try:
                        self.agents['pubmed'] = PubMedQueryGenerator(openai_client=openai_client)
                        agents_initialized.append('pubmed')
                        logger.info("[DEBUG] Agente 'pubmed' inicializado")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error inicializando agente PubMed: {e}")
                
                if llm_instance and db_path:
                    try:
                        medgemma_agent = None
                        try:
                            medgemma_agent = MedGemmaClinicalAgent()
                            logger.info("‚úÖ Agente MedGemma creado para an√°lisis cl√≠nico")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Error creando agente MedGemma: {e}")
                        
                        sql_agent_instance = SQLAgentIntelligentEnhanced(
                            db_path=db_path, 
                            llm=llm_instance,
                            medgemma_agent=None  # Se asignar√° despu√©s si est√° disponible
                        )
                        self.agents['sql'] = sql_agent_instance
                        agents_initialized.append('sql')
                        logger.info("[DEBUG] Agente 'sql' inicializado")
                        
                        try:
                            self.agents['fhir'] = FHIRMedicalAgent(db_path=db_path, llm=llm_instance, sql_agent=sql_agent_instance, medgemma_agent=None)  # type: ignore
                            agents_initialized.append('fhir')
                            logger.info("[DEBUG] Agente 'fhir' inicializado")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Error inicializando agente FHIR: {e}")
                        
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error inicializando agente SQL Enhanced: {e}")
                
                if llm_instance and db_path:
                    try:
                        self.agents['fhir_persistence'] = FHIRPersistenceAgent(db_path=db_path, llm_client=llm_instance)
                        agents_initialized.append('fhir_persistence')
                        logger.info("[DEBUG] Agente 'fhir_persistence' inicializado")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error inicializando agente de persistencia: {e}")
                
                # Inicializar agente MedGemma Cl√≠nico
                try:
                    medgemma_agent_instance = MedGemmaClinicalAgent(llm=llm_instance)
                    self.agents['clinical_analysis'] = medgemma_agent_instance
                    agents_initialized.append('clinical_analysis')
                    logger.info(f"‚úÖ Agente MedGemma Cl√≠nico inicializado: {medgemma_agent_instance}")
                    logger.info(f"[DEBUG] self.agents['clinical_analysis']: {self.agents.get('clinical_analysis')}")
                    # Pasar MedGemma a los agentes para an√°lisis cl√≠nico integrado
                    if 'sql' in self.agents:
                        self.agents['sql'].medgemma_agent = medgemma_agent_instance
                        logger.info("‚úÖ MedGemma integrado con agente SQL")
                    if 'biochat' in self.agents:
                        self.agents['biochat'].medgemma_agent = medgemma_agent_instance
                        logger.info("‚úÖ MedGemma integrado con agente BioChat")
                    if 'fhir' in self.agents:
                        self.agents['fhir'].medgemma_agent = medgemma_agent_instance
                        logger.info("‚úÖ MedGemma integrado con agente FHIR")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error inicializando agente MedGemma: {e}")
                
                if agents_initialized:
                    logger.info(f"‚úÖ {len(agents_initialized)} agentes v2.0 inicializados: {', '.join(agents_initialized)}")
                    logger.info(f"[DEBUG] Agentes disponibles tras inicializaci√≥n: {list(self.agents.keys())}")
                else:
                    logger.warning("‚ö†Ô∏è No se pudo inicializar ning√∫n agente v2.0")
            else:
                logger.warning("‚ö†Ô∏è Agentes v2.0 no disponibles - sistema limitado.")
        except Exception as e:
            logger.error(f"‚ùå Error cr√≠tico inicializando agentes: {e}")
        logger.info(f"üìä Agentes disponibles: {list(self.agents.keys())}")

    def _find_database(self) -> Optional[str]:
        # FORZAR uso de database_new.sqlite3.db espec√≠ficamente
        target_db = 'database_new.sqlite3.db'
        
        if os.path.exists(target_db):
            logger.info(f"‚úÖ Base de datos encontrada: {target_db}")
            return target_db
        
        # Fallback a rutas relativas si no existe en el directorio actual
        possible_paths = [
            os.path.join(os.path.dirname(__file__), '..', '..', 'database_new.sqlite3.db'),
            os.path.join(os.getcwd(), 'database_new.sqlite3.db'),
            './database_new.sqlite3.db',
            '../database_new.sqlite3.db',
            '../../database_new.sqlite3.db'
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                logger.info(f"‚úÖ Base de datos encontrada (fallback): {path}")
                return path
        
        logger.error("‚ùå Base de datos no encontrada")
        return None
    
    async def classify_query_ultra_fast(self, query: str) -> Tuple[str, float]:
        start_time = time.time()
        
        query_clean = query.strip().lower()
        
        # 1. Detecci√≥n r√°pida de saludos b√°sicos (sin LLM)
        simple_greetings = {
            "hola", "hi", "hello", "hey", "buenos d√≠as", "buenas tardes", 
            "buenas noches", "good morning", "good afternoon", "good evening",
            "¬øqu√© tal?", "que tal", "c√≥mo est√°s", "como estas", "¬øc√≥mo est√°?",
            "como esta", "saludos", "gracias", "thank you", "thanks", "ok", "vale",
            "que puedes hacer", "qu√© puedes hacer por mi", "que puedes hacer por mi",
            "qu√© puedes hacer por mi", "que sabes hacer", "qu√© sabes hacer"
        }
        
        # Normalizar la consulta para comparaci√≥n
        query_normalized = query_clean.replace("?", "").replace("¬ø", "").strip()
        
        if query_normalized in simple_greetings:
            return "greeting", 0.99
        
        # 2. Clasificaci√≥n inteligente con LLM
        if self.llm_classifier:
            try:
                agent, confidence = await self._classify_with_intelligent_llm(query)
                if confidence >= 0.6:  # Umbral de confianza
                    if self.metrics:
                        self.metrics.llm_calls += 1
                    return agent, confidence
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error en clasificaci√≥n LLM: {e}")
        
        # 3. Fallback inteligente sin patrones hardcodeados
        agent, confidence = await self._smart_fallback_classification(query)
        
        if self.metrics:
            self.metrics.classification_time += time.time() - start_time
            self.metrics.local_classifications += 1
        
        return agent, confidence

    async def _classify_with_intelligent_llm(self, query: str) -> Tuple[str, float]:
        """Clasificaci√≥n inteligente usando LLM con prompt espec√≠fico"""
        
        if not self.llm_classifier:
            logger.warning("‚ö†Ô∏è LLM no disponible para clasificaci√≥n inteligente")
            return "greeting", 0.3
        
        prompt = f'''
Analiza esta consulta m√©dica y determina qu√© tipo de agente especializado debe manejarla.

CONSULTA: "{query}"

TIPOS DE AGENTES DISPONIBLES:

1. GREETING: Para saludos, agradecimientos, preguntas sobre capacidades del sistema
2. BIOCHAT: Para b√∫squedas de literatura m√©dica, ensayos cl√≠nicos, art√≠culos cient√≠ficos, investigaci√≥n biom√©dica, estudios recientes, papers cient√≠ficos
3. SQL: Para consultas de datos de pacientes, estad√≠sticas m√©dicas, informaci√≥n de base de datos cl√≠nica, b√∫squeda de pacientes
4. FHIR: Para gesti√≥n de registros m√©dicos, notas cl√≠nicas, informaci√≥n de pacientes
5. CLINICAL_ANALYSIS: Para an√°lisis cl√≠nico b√°sico, explicaciones m√©dicas simples, conceptos m√©dicos, medicamentos, diagn√≥sticos

CRITERIOS DE CLASIFICACI√ìN DETALLADOS:

- CLINICAL_ANALYSIS: 
  * Explicaciones b√°sicas de enfermedades ("¬øQu√© es la diabetes?")
  * Conceptos m√©dicos simples ("¬øQu√© es la hipertensi√≥n?")
  * Informaci√≥n de medicamentos b√°sica ("¬øPara qu√© sirve la metformina?")
  * S√≠ntomas y diagn√≥sticos simples ("¬øCu√°les son los s√≠ntomas de la gripe?")
  * Interacciones farmacol√≥gicas b√°sicas ("¬øPuedo tomar paracetamol con ibuprofeno?")

- BIOCHAT: 
  * B√∫squedas de investigaci√≥n ("¬øQu√© estudios recientes hay sobre...?")
  * Ensayos cl√≠nicos ("¬øCu√°les son los √∫ltimos ensayos sobre...?")
  * Literatura cient√≠fica ("¬øQu√© papers hay sobre...?")
  * Investigaci√≥n biom√©dica ("¬øQu√© investigaciones recientes...?")
  * Evidencia cient√≠fica ("¬øQu√© evidencia hay sobre...?")
  * Mecanismos moleculares ("¬øCu√°les son los mecanismos moleculares...?")
  * Estudios avanzados ("¬øCu√°l es el estado actual de la investigaci√≥n...?")

- SQL: 
  * Datos de pacientes ("Busca pacientes con...")
  * Estad√≠sticas ("¬øCu√°ntos pacientes...?")
  * Informaci√≥n de base de datos ("Mu√©strame todos los...")
  * B√∫squedas espec√≠ficas ("¬øCu√°l es el √∫ltimo paciente?")

- FHIR: 
  * Gesti√≥n de registros ("Registrar paciente...")
  * Notas cl√≠nicas ("Crear nota cl√≠nica...")
  * Informaci√≥n estructurada de pacientes

- GREETING: 
  * Saludos ("Hola", "¬øQu√© puedes hacer?")
  * Preguntas sobre el sistema

Ejemplos espec√≠ficos:
- "¬øQu√© es la diabetes?" ‚Üí CLINICAL_ANALYSIS
- "¬øPara qu√© sirve la metformina?" ‚Üí CLINICAL_ANALYSIS
- "¬øCu√°les son los s√≠ntomas de la hipertensi√≥n?" ‚Üí CLINICAL_ANALYSIS
- "¬øQu√© investigaciones recientes hay sobre diabetes?" ‚Üí BIOCHAT
- "¬øCu√°les son los √∫ltimos ensayos cl√≠nicos sobre CAR-T?" ‚Üí BIOCHAT
- "¬øQu√© estudios hay sobre nuevos tratamientos para diabetes?" ‚Üí BIOCHAT
- "Busca pacientes con diabetes" ‚Üí SQL
- "¬øCu√°ntos pacientes hay?" ‚Üí SQL
- "¬øCu√°l es el √∫ltimo paciente?" ‚Üí SQL
- "Registrar paciente Juan P√©rez" ‚Üí FHIR
- "Hola" ‚Üí GREETING

Responde SOLO con este JSON:
{{
  "agent": "greeting|biochat|sql|fhir|clinical_analysis",
  "confidence": 0.0-1.0,
  "reasoning": "explicaci√≥n breve de la clasificaci√≥n"
}}
'''
        
        try:
            response = await self.llm_classifier.ainvoke(prompt)
            content = str(response.content)
            
            # Extraer JSON de la respuesta
            import re
            json_match = re.search(r'\{[\s\S]*\}', content)
            if json_match:
                result = json.loads(json_match.group(0))
                agent = result.get("agent", "greeting").lower()
                confidence = float(result.get("confidence", 0.5))
                reasoning = result.get("reasoning", "")
                
                # Validar agente
                valid_agents = ["greeting", "sql", "fhir", "biochat"]
                if agent not in valid_agents:
                    agent = "greeting"
                    confidence = 0.3
                
                logger.info(f"üß† LLM clasific√≥ como '{agent}' (confianza: {confidence:.2f}) - Raz√≥n: {reasoning}")
                return agent, confidence
            else:
                logger.warning("‚ö†Ô∏è No se encontr√≥ JSON v√°lido en respuesta del LLM")
                return "greeting", 0.3
                
        except Exception as e:
            logger.error(f"‚ùå Error en clasificaci√≥n LLM: {e}")
            return "greeting", 0.3

    async def _smart_fallback_classification(self, query: str) -> Tuple[str, float]:
        """Clasificaci√≥n inteligente usando LLM con prompt espec√≠fico para fallback"""
        
        if not self.llm_classifier:
            logger.warning("‚ö†Ô∏è LLM no disponible para clasificaci√≥n de fallback")
            return "greeting", 0.3
        
        prompt = f'''
Eres un clasificador inteligente de consultas m√©dicas. Analiza la siguiente consulta y determina qu√© tipo de agente especializado debe manejarla.

CONSULTA: "{query}"

TIPOS DE AGENTES DISPONIBLES:

1. GREETING: Para saludos, agradecimientos, preguntas sobre capacidades del sistema
2. BIOCHAT: Para b√∫squedas de literatura m√©dica, ensayos cl√≠nicos, art√≠culos cient√≠ficos, investigaci√≥n biom√©dica, estudios recientes, papers cient√≠ficos
3. SQL: Para consultas de datos de pacientes, estad√≠sticas m√©dicas, informaci√≥n de base de datos cl√≠nica, b√∫squeda de pacientes
4. FHIR: Para gesti√≥n de registros m√©dicos, notas cl√≠nicas, informaci√≥n de pacientes
5. CLINICAL_ANALYSIS: Para an√°lisis cl√≠nico b√°sico, explicaciones m√©dicas simples, conceptos m√©dicos, medicamentos, diagn√≥sticos

CRITERIOS DE CLASIFICACI√ìN DETALLADOS:

- CLINICAL_ANALYSIS: 
  * Explicaciones b√°sicas de enfermedades ("¬øQu√© es la diabetes?")
  * Conceptos m√©dicos simples ("¬øQu√© es la hipertensi√≥n?")
  * Informaci√≥n de medicamentos b√°sica ("¬øPara qu√© sirve la metformina?")
  * S√≠ntomas y diagn√≥sticos simples ("¬øCu√°les son los s√≠ntomas de la gripe?")
  * Interacciones farmacol√≥gicas b√°sicas ("¬øPuedo tomar paracetamol con ibuprofeno?")

- BIOCHAT: 
  * B√∫squedas de investigaci√≥n ("¬øQu√© estudios recientes hay sobre...?")
  * Ensayos cl√≠nicos ("¬øCu√°les son los √∫ltimos ensayos sobre...?")
  * Literatura cient√≠fica ("¬øQu√© papers hay sobre...?")
  * Investigaci√≥n biom√©dica ("¬øQu√© investigaciones recientes...?")
  * Evidencia cient√≠fica ("¬øQu√© evidencia hay sobre...?")
  * Mecanismos moleculares ("¬øCu√°les son los mecanismos moleculares...?")
  * Estudios avanzados ("¬øCu√°l es el estado actual de la investigaci√≥n...?")

- SQL: 
  * Datos de pacientes ("Busca pacientes con...")
  * Estad√≠sticas ("¬øCu√°ntos pacientes...?")
  * Informaci√≥n de base de datos ("Mu√©strame todos los...")
  * B√∫squedas espec√≠ficas ("¬øCu√°l es el √∫ltimo paciente?")

- FHIR: 
  * Gesti√≥n de registros ("Registrar paciente...")
  * Notas cl√≠nicas ("Crear nota cl√≠nica...")
  * Informaci√≥n estructurada de pacientes

- GREETING: 
  * Saludos ("Hola", "¬øQu√© puedes hacer?")
  * Preguntas sobre el sistema

Ejemplos espec√≠ficos:
- "¬øQu√© es la diabetes?" ‚Üí CLINICAL_ANALYSIS
- "¬øPara qu√© sirve la metformina?" ‚Üí CLINICAL_ANALYSIS
- "¬øQu√© investigaciones recientes hay sobre diabetes?" ‚Üí BIOCHAT
- "¬øCu√°les son los √∫ltimos ensayos cl√≠nicos sobre CAR-T?" ‚Üí BIOCHAT
- "Busca pacientes con diabetes" ‚Üí SQL
- "¬øCu√°ntos pacientes hay?" ‚Üí SQL
- "Registrar paciente Juan P√©rez" ‚Üí FHIR
- "Hola" ‚Üí GREETING

Responde SOLO con este JSON:
{{
  "agent": "greeting|biochat|sql|fhir|clinical_analysis",
  "confidence": 0.0-1.0,
  "reasoning": "explicaci√≥n breve de la clasificaci√≥n"
}}
'''
        
        try:
            response = await self.llm_classifier.ainvoke(prompt)
            content = str(response.content)
            
            # Extraer JSON de la respuesta
            import re
            json_match = re.search(r'\{[\s\S]*\}', content)
            if json_match:
                result = json.loads(json_match.group(0))
                agent = result.get("agent", "greeting").lower()
                confidence = float(result.get("confidence", 0.5))
                reasoning = result.get("reasoning", "")
                
                # Validar agente
                valid_agents = ["greeting", "sql", "fhir", "biochat"]
                if agent not in valid_agents:
                    agent = "greeting"
                    confidence = 0.3
                
                logger.info(f"üß† LLM Fallback clasific√≥ como '{agent}' (confianza: {confidence:.2f}) - Raz√≥n: {reasoning}")
                return agent, confidence
            else:
                logger.warning("‚ö†Ô∏è No se encontr√≥ JSON v√°lido en respuesta del LLM de fallback")
                return "greeting", 0.3
                
        except Exception as e:
            logger.error(f"‚ùå Error en clasificaci√≥n LLM de fallback: {e}")
            return "greeting", 0.3

    def _classify_with_basic_patterns(self, query: str) -> Tuple[str, float]:
        """ELIMINADO - Ya no usamos patrones hardcodeados"""
        # Este m√©todo se mantiene por compatibilidad pero ya no se usa
        return "greeting", 0.3
    
    async def process_query_optimized(self, query: str, stream_callback=None) -> Dict[str, Any]:
        start_time = time.time()
        
        if self.metrics:
            self.metrics.total_queries += 1
        
        if self.enable_cache and self.query_cache:
            query_hash = hash(query.lower().strip())
            if query_hash in self.query_cache:
                cached = self.query_cache[query_hash]
                if not cached.is_expired(self.cache_ttl_minutes):
                    return {
                        "success": True,
                        "agent_type": cached.agent_type,
                        "message": cached.result,
                        "confidence": cached.confidence,
                        "from_cache": True
                    }
        
        mem_context = self._get_memory_context()
        augmented_query = f"{mem_context}\n{query}" if mem_context else query

        agent_type, confidence = await self.classify_query_ultra_fast(augmented_query)
        
        if stream_callback:
            stream_callback(f"üéØ Derivando a: {agent_type}")
        
        try:
            if agent_type in self.agents and self.agents[agent_type] is not None:
                agent = self.agents[agent_type]
                
                context = self._get_user_memory_context()
                
                # Llamada al agente con manejo mejorado de errores
                try:
                    if asyncio.iscoroutinefunction(agent.process_query):
                        result = await agent.process_query(query=query, context=context)
                    else:
                        result = agent.process_query(query=query, context=context)
                    
                    # Validar que el resultado no sea None
                    if result is None:
                        result = {"success": False, "message": "El agente no devolvi√≥ una respuesta v√°lida"}
                    
                    # Asegurar que el resultado sea un diccionario
                    if not isinstance(result, dict):
                        result = {"success": True, "message": str(result)}
                    
                except Exception as agent_error:
                    logger.error(f"‚ùå Error ejecutando agente {agent_type}: {agent_error}")
                    result = {
                        "success": False, 
                        "message": f"Error en el agente {agent_type}: {str(agent_error)}"
                    }

            else:
                result = {
                    "success": False,
                    "message": f"Agente '{agent_type}' no disponible."
                }
            
            # Extraer el mensaje limpio
            clean_message = self._extract_clean_message(result, agent_type)
            
            # Crear respuesta estructurada
            response_dict = {
                "success": result.get("success", True),
                "agent_type": agent_type,
                "confidence": confidence,
                "message": clean_message,
                "original_result": result,
                "processing_time": time.time() - start_time
            }
            
            if self.enable_cache and self.query_cache is not None:
                self.query_cache[query_hash] = CacheEntry(str(query_hash), agent_type, confidence, clean_message, datetime.now())
            
            self._update_conversation_memory_v2(query, agent_type, result)
            self._update_user_memory(result)
            
            return response_dict
                
        except Exception as e:
            logger.error(f"‚ùå Error cr√≠tico procesando consulta: {e}")
            return {
                "success": False,
                "agent_type": "unknown",
                "confidence": 0.0,
                "message": f"‚ùå Error procesando consulta: {str(e)}",
                "processing_time": time.time() - start_time
            }
    
    def _format_response_v2(self, result: Any, agent_type: str, confidence: float, query: str) -> str:
        clean_message = self._extract_clean_message(result, agent_type)
        # Preservar saltos de l√≠nea y aplicar colores
        formatted_message = self._apply_agent_colors(clean_message, agent_type)
        # Asegurar que los saltos de l√≠nea se mantengan
        return formatted_message.replace('\\n', '\n')
            
    def _extract_clean_message(self, result: Any, agent_type: str) -> str:
        """Extrae el mensaje limpio del resultado del agente"""
        if isinstance(result, dict):
            # PRIORIDAD 1: Usar formatted_data si est√° disponible (SQL Agent mejorado)
            if 'formatted_data' in result and result['formatted_data']:
                return str(result['formatted_data'])
            
            # PRIORIDAD 2: Buscar diferentes claves posibles para el mensaje
            message_keys = ['message', 'response_text', 'response', 'content', 'text']
            for key in message_keys:
                if key in result and result[key]:
                    return str(result[key])
            
            # PRIORIDAD 3: Si es SQL Agent y tiene datos, formatear b√°sicamente
            if agent_type == 'sql' and 'data' in result and result['data']:
                return self._format_sql_data_basic(result['data'])
            
            # PRIORIDAD 4: Si es SQL Agent y tiene SQL pero no datos, mostrar mensaje informativo
            if agent_type == 'sql' and 'sql' in result:
                if result.get('success'):
                    return "‚úÖ Consulta SQL ejecutada correctamente. Los resultados se han procesado."
                else:
                    return f"‚ùå Error en la consulta SQL: {result.get('error', 'Error desconocido')}"
            
            # PRIORIDAD 5: Si es an√°lisis cl√≠nico con MedGemma
            if 'clinical_analysis' in result and result['clinical_analysis']:
                analysis = result['clinical_analysis']
                if isinstance(analysis, dict) and 'analysis' in analysis:
                    return f"üè• An√°lisis cl√≠nico: {analysis['analysis']}"
            
            # Si no hay mensaje espec√≠fico, usar todo el resultado
            return str(result)
        elif isinstance(result, str):
            return result
        else:
            return str(result)
    
    def _format_sql_data_basic(self, data: List[Dict[str, Any]]) -> str:
        """Formatea datos SQL de manera b√°sica si no hay formatted_data"""
        if not data:
            return "üìä **No se encontraron resultados**"
        
        formatted = []
        formatted.append("üìä **RESULTADOS ENCONTRADOS**")
        formatted.append("")
        
        for i, item in enumerate(data[:10], 1):  # Mostrar m√°ximo 10
            if isinstance(item, dict):
                item_str = " | ".join([f"{k}: {v}" for k, v in item.items()])
                formatted.append(f"   {i}. {item_str}")
            else:
                formatted.append(f"   {i}. {item}")
        
        if len(data) > 10:
            formatted.append("")
            formatted.append(f"üìÑ *... y {len(data) - 10} resultados m√°s*")
        
        return "\n".join(formatted)
    
    def _apply_agent_colors(self, message: str, agent_type: str) -> str:
        colors = {'greeting': Colors.CYAN, 'sql': Colors.GREEN, 'fhir': Colors.BLUE, 'biochat': Colors.PURPLE, 'clinical_analysis': Colors.YELLOW}
        icons = {'greeting': 'üëã', 'sql': 'üóÑÔ∏è', 'fhir': 'üè•', 'biochat': 'üìö', 'clinical_analysis': 'üè•'}
        color = colors.get(agent_type, Colors.END)
        icon = icons.get(agent_type, 'ü§ñ')
        return f"{color}{icon} {message}{Colors.END}"
    
    def _update_conversation_memory_v2(self, query: str, agent: str, result: Any):
        self.conversation_history.append({'query': query, 'agent': agent, 'result_summary': str(result)[:100]})
        if len(self.conversation_history) > self.flexible_config.get('max_conversation_history', 20):
            self.conversation_history.pop(0)
    
    def _extract_result_summary_v2(self, result: Any) -> str:
        if isinstance(result, dict) and 'summary' in result:
            return result['summary']
        return str(result)[:100]
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        if not self.metrics:
            return {"error": "M√©tricas no habilitadas"}
        
        return {
            "total_queries": self.metrics.total_queries,
            "cache_hit_rate": f"{self.metrics.cache_hit_rate:.1%}",
            "avg_response_time": f"{self.metrics.avg_response_time:.3f}s",
            "agent_usage": self.metrics.agent_usage
        }

    def _log_initialization_summary(self):
        logger.info("üìä RESUMEN DE INICIALIZACI√ìN:")
        logger.info(f"   ‚ö° Cache: {'‚úÖ Habilitado' if self.enable_cache else '‚ùå Deshabilitado'}")
        logger.info(f"   üìà M√©tricas: {'‚úÖ Habilitadas' if self.enable_performance_monitoring else '‚ùå Deshabilitadas'}")
        logger.info(f"   ü§ñ LLM: {'‚úÖ Disponible' if self.llm_classifier else '‚ùå No disponible'}")

    async def _looks_like_clinical_note(self, text: str) -> bool:
        """
        üß† CLASIFICACI√ìN SEM√ÅNTICA INTELIGENTE
        Usa LLM para determinar si es una nota cl√≠nica bas√°ndose en el significado, no en patrones
        """
        # Si el texto es muy corto, no es nota cl√≠nica
        if len(text.split()) < 10:
            return False
            
        # Si no tenemos LLM, usar heur√≠stica simple pero flexible
        if not self.llm_classifier:
            return self._simple_clinical_note_heuristic(text)
        
        try:
            # PROMPT INTELIGENTE para clasificaci√≥n sem√°ntica
            prompt = f"""
Eres un clasificador experto de documentos m√©dicos. Tu tarea es determinar si el siguiente texto es una NOTA CL√çNICA que debe procesarse con el agente FHIR.

DEFINICI√ìN DE NOTA CL√çNICA:
- Documento que registra informaci√≥n m√©dica de un paciente
- Contiene diagn√≥sticos, tratamientos, medicamentos, signos vitales
- Puede tener formato estructurado (SOAP, H&P) o narrativo
- Es informaci√≥n para REGISTRAR, no para CONSULTAR

EJEMPLOS DE NOTAS CL√çNICAS:
‚úÖ "S ‚Äì Subjetivo: Paciente de 45 a√±os con dolor lumbar..."
‚úÖ "Control de paciente diab√©tico. HbA1c 7.2%. Continuar metformina..."
‚úÖ "Paciente presenta fiebre 38.5¬∞C. Prescribir paracetamol..."
‚úÖ "Motivo de consulta: Dolor tor√°cico. Diagn√≥stico: Angina..."

EJEMPLOS DE NO NOTAS CL√çNICAS:
‚ùå "¬øQu√© medicamentos toma el paciente 1010?"
‚ùå "¬øCu√°ntos pacientes con diabetes tenemos?"
‚ùå "Buscar informaci√≥n sobre el paciente Juan"
‚ùå "Mostrar las alergias del paciente 123"

TEXTO A ANALIZAR:
"{text}"

Responde SOLO con "SI" si es una nota cl√≠nica para registrar, o "NO" si no lo es.
"""
            
            response = await self.llm_classifier.ainvoke(prompt)
            result = str(response.content).strip().upper()
            
            is_clinical_note = "SI" in result or "YES" in result
            logger.info(f"üß† Clasificaci√≥n sem√°ntica: {'NOTA CL√çNICA' if is_clinical_note else 'NO ES NOTA CL√çNICA'}")
            
            return is_clinical_note
            
        except Exception as e:
            logger.error(f"Error en clasificaci√≥n sem√°ntica: {e}")
            return self._simple_clinical_note_heuristic(text)
    
    def _simple_clinical_note_heuristic(self, text: str) -> bool:
        """
        Heur√≠stica simple de respaldo cuando no hay LLM disponible
        """
        text_lower = text.lower()
        
        # Detectar si es una pregunta
        question_indicators = ['¬ø', '?', 'cu√°ntos', 'cu√°l', 'qu√©', 'd√≥nde', 'cu√°ndo', 'c√≥mo']
        is_question = any(indicator in text_lower for indicator in question_indicators)
        
        if is_question:
            return False  # Las preguntas van a SQL, no a FHIR
        
        # Detectar informaci√≥n m√©dica estructurada
        medical_indicators = [
            'diagn√≥stico', 'tratamiento', 'medicaci√≥n', 'prescribir', 'mg/', 'mmhg',
            'signos vitales', 'exploraci√≥n f√≠sica', 'antecedentes', 'evoluci√≥n'
        ]
        
        medical_matches = sum(1 for indicator in medical_indicators if indicator in text_lower)
        
        # Es nota cl√≠nica si tiene informaci√≥n m√©dica Y NO es pregunta Y es texto largo
        return medical_matches >= 2 and len(text.split()) > 20

    def _get_memory_context(self) -> str:
        mem = self.user_memories.get(self.current_user_id)
        if not mem: return ""
        parts = []
        if mem.last_patient_id: parts.append(f"ultimo_paciente_id={mem.last_patient_id}")
        if mem.last_tables: parts.append(f"tablas_recientes={','.join(mem.last_tables[:3])}")
        return "MEMORIA_CONTEXTUAL: " + "; ".join(parts) if parts else ""

    def _update_user_memory(self, result: Any):
        if not isinstance(result, dict): return
        mem = self.user_memories.setdefault(self.current_user_id, UserMemory())
        pid = result.get('patient_id')
        if pid:
            mem.last_patient_id = str(pid)
        mem.updated_at = datetime.utcnow()
        save_memory(self.user_memories)

    def _get_user_memory_context(self) -> Dict[str, Any]:
        mem = self.user_memories.get(self.current_user_id)
        if mem and mem.last_patient_id:
            return {'last_patient_id': mem.last_patient_id}
        return {}
        
class IntelligentOrchestratorV3(FlexibleOrchestrator):
    """
    üß† Orquestador Inteligente V3 - Enrutamiento Basado en Intenci√≥n
    ================================================================
    Esta versi√≥n reemplaza la clasificaci√≥n por patrones con un sistema de
    enrutamiento de dos etapas basado en an√°lisis de intenci√≥n y puntuaci√≥n de agentes.
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        logger.info("üöÄ Iniciando IntelligentOrchestratorV3 con enrutamiento avanzado.")
        # Limpiar patrones de fallback para forzar el nuevo sistema
        self.basic_fallback_patterns = {}
        self.use_intelligent_classification = True

    async def process_query_optimized(self, query: str, stream_callback=None) -> Union[Dict[str, Any], str]:
        """
        VERSI√ìN SIMPLIFICADA: Procesamiento directo con clasificaci√≥n LLM
        """
        start_time = time.time()
        
        # 0. DETECCI√ìN R√ÅPIDA DE NOTA CL√çNICA (mejorada)
        if await self._looks_like_clinical_note(query):
            agent_name, best_score, justification = "fhir", 0.99, "Detectado formato de nota cl√≠nica"
            logger.info("üìã Nota cl√≠nica detectada, usando FHIR")
        else:
            # 1. CLASIFICACI√ìN SIMPLE CON LLM
            logger.info("üß† Clasificando consulta con LLM...")
            intent_analysis = await self._analyze_intent_and_entities(query)
            agent_name, best_score, justification = await self._score_and_select_agent(intent_analysis, query)
        
        # Mostrar agente seleccionado
        agent_icons = {
            'greeting': 'üëã',
            'sql': 'üóÑÔ∏è', 
            'fhir': 'üìã',
            'biochat': 'üî¨',
            'clinical_analysis': 'üß†'
        }
        
        agent_names = {
            'greeting': 'Saludo',
            'sql': 'Base de Datos',
            'fhir': 'Registros M√©dicos', 
            'biochat': 'Investigaci√≥n Biom√©dica',
            'clinical_analysis': 'An√°lisis Cl√≠nico'
        }
        
        icon = agent_icons.get(agent_name, 'ü§ñ')
        name = agent_names.get(agent_name, 'Agente')
        
        if stream_callback:
            stream_callback(f"üéØ {icon} {name}")
        
        # 2. EJECUCI√ìN DEL AGENTE
        agent = self.agents.get(agent_name)
        if not agent:
            return {
                'success': False,
                'agent_type': agent_name,
                'confidence': 0.0,
                'message': f"Agente '{agent_name}' no disponible. Agentes disponibles: {list(self.agents.keys())}",
                'data': [],
                'processing_time': time.time() - start_time
            }
            
        # Pasar contexto de memoria al agente si es necesario
        context = self._get_user_memory_context()
            
        # Ejecutar agente
        try:
            import inspect
            sig = inspect.signature(agent.process_query)
            
            call_kwargs: Dict[str, Any] = {'query': query}
            if 'stream_callback' in sig.parameters:
                call_kwargs['stream_callback'] = stream_callback
            if 'context' in sig.parameters:
                call_kwargs['context'] = context

            if asyncio.iscoroutinefunction(agent.process_query):
                result = await agent.process_query(**call_kwargs)
            else:
                result = agent.process_query(**call_kwargs)

        except Exception as e:
            logger.error(f"‚ùå Error ejecutando el agente '{agent_name}': {e}", exc_info=True)
            result = {
                'success': False,
                'message': f"Error al procesar la consulta con el agente {agent_name}: {str(e)}"
            }

        # Actualizar memoria
        self._update_conversation_memory_v2(query, agent_name, result)
        self._update_user_memory(result)
        
        # Extraer el mensaje limpio
        clean_message = self._extract_clean_message(result, agent_name)
        
        # Crear respuesta estructurada
        response_dict = {
            'success': result.get('success', True) if isinstance(result, dict) else True,
            'agent_type': agent_name,
            'confidence': best_score,
            'message': clean_message,
            'original_result': result,
            'processing_time': time.time() - start_time
        }
        
        return response_dict

    async def _analyze_intent_and_entities(self, query: str) -> Dict[str, Any]:
        """
        SISTEMA DE CLASIFICACI√ìN CON M√öLTIPLES PROMPTS ESPEC√çFICOS
        Cada prompt explica al LLM qu√© hace cada agente espec√≠ficamente
        """
        if not self.llm_classifier:
            return {'intent': 'unknown', 'entities': {}, 'suggested_agent': 'greeting', 'justification': 'LLM no disponible'}

        # PROMPT 1: EXPLICACI√ìN DE CADA AGENTE
        agent_explanation_prompt = f"""
Eres un clasificador experto de consultas m√©dicas. Tu tarea es entender qu√© agente debe manejar cada consulta.

CONSULTA: "{query}"

EXPLICACI√ìN DE CADA AGENTE:

1. **GREETING AGENT** - Para conversaci√≥n general y saludos
   - Saludos: "hola", "buenos d√≠as", "gracias"
   - Preguntas sobre el sistema: "¬øqu√© puedes hacer?", "ayuda"
   - Conversaci√≥n casual: "¬øc√≥mo est√°s?", "ok", "vale"

2. **CLINICAL_ANALYSIS AGENT** - Para explicaciones m√©dicas y medicamentos
   - Preguntas sobre medicamentos: "qu√© es la azitromicina", "para qu√© sirve el paracetamol"
   - Explicaciones m√©dicas: "explica qu√© es la diabetes", "qu√© es la hipertensi√≥n"
   - Conceptos m√©dicos: "qu√© significa anemia", "explica el concepto de diabetes"
   - Interacciones farmacol√≥gicas: "¬øpuedo tomar paracetamol con ibuprofeno?"

3. **BIOCHAT AGENT** - Para b√∫squedas de investigaci√≥n cient√≠fica
   - Estudios cient√≠ficos: "estudios sobre diabetes", "investigaciones recientes"
   - Ensayos cl√≠nicos: "ensayos cl√≠nicos sobre c√°ncer", "√∫ltimos estudios"
   - Literatura m√©dica: "papers sobre diabetes", "evidencia cient√≠fica"
   - Investigaci√≥n: "investigaciones sobre tratamientos", "art√≠culos cient√≠ficos"

4. **SQL AGENT** - Para consultas de base de datos de pacientes
   - B√∫squedas de pacientes: "buscar Ana Garc√≠a", "datos de Juan"
   - Estad√≠sticas: "cu√°ntos pacientes", "mostrar pacientes"
   - Informaci√≥n de BD: "listar datos", "informaci√≥n de pacientes"

5. **FHIR AGENT** - Para registrar informaci√≥n m√©dica
   - Notas cl√≠nicas: "procesar nota cl√≠nica", "registrar paciente"
   - Datos m√©dicos: "crear paciente", "insertar diagn√≥stico"

Responde SOLO con el nombre del agente m√°s apropiado: greeting|clinical_analysis|biochat|sql|fhir
"""

        try:
            # PRIMERA CLASIFICACI√ìN
            response1 = await self.llm_classifier.ainvoke(agent_explanation_prompt)
            agent1 = self._extract_response_text(response1).strip().lower().replace('.', '').replace('"', '').replace("'", "")
            
            # Validar primera clasificaci√≥n
            valid_agents = ["greeting", "clinical_analysis", "biochat", "sql", "fhir"]
            if agent1 not in valid_agents:
                agent1 = "greeting"
            
            logger.info(f"üß† Primera clasificaci√≥n: {agent1}")
            
            # PROMPT 2: CONFIRMACI√ìN ESPEC√çFICA PARA CONSULTAS AMBIGUAS
            if agent1 in ["clinical_analysis", "biochat"]:
                confirmation_prompt = f"""
CONSULTA: "{query}"
CLASIFICACI√ìN PREVIA: {agent1}

Para confirmar la clasificaci√≥n, analiza espec√≠ficamente:

Si es CLINICAL_ANALYSIS:
- Preguntas sobre medicamentos espec√≠ficos ("qu√© es la azitromicina")
- Explicaciones de conceptos m√©dicos b√°sicos ("qu√© es la diabetes")
- Informaci√≥n m√©dica general para pacientes

Si es BIOCHAT:
- B√∫squedas de estudios cient√≠ficos ("estudios sobre diabetes")
- Investigaciones recientes ("√∫ltimas investigaciones")
- Literatura m√©dica avanzada ("papers cient√≠ficos")

¬øLa consulta busca informaci√≥n m√©dica b√°sica (clinical_analysis) o investigaci√≥n cient√≠fica (biochat)?

Responde SOLO: clinical_analysis|biochat
"""
                try:
                    response2 = await self.llm_classifier.ainvoke(confirmation_prompt)
                    agent2 = self._extract_response_text(response2).strip().lower().replace('.', '').replace('"', '').replace("'", "")
                    
                    if agent2 in ["clinical_analysis", "biochat"]:
                        final_agent = agent2
                        logger.info(f"üß† Confirmaci√≥n: {agent2}")
                    else:
                        final_agent = agent1
                        logger.info(f"üß† Manteniendo clasificaci√≥n original: {agent1}")
                except:
                    final_agent = agent1
            else:
                final_agent = agent1
            
            # PROMPT 3: DETECCI√ìN ESPECIAL PARA NOTAS CL√çNICAS Y CASOS COMPLEJOS
            if len(query.split()) > 20:  # Textos largos pueden ser notas cl√≠nicas
                clinical_note_prompt = f"""
CONSULTA: "{query}"

Analiza si este texto es una NOTA CL√çNICA que debe procesarse con el agente FHIR.

Una NOTA CL√çNICA contiene:
- Informaci√≥n de paciente (nombre, edad, fecha)
- Diagn√≥sticos m√©dicos
- Tratamientos prescritos
- Signos vitales
- Observaciones m√©dicas
- Formato m√©dico estructurado

Ejemplos de NOTAS CL√çNICAS:
‚úÖ "Paciente Mar√≠a Gonz√°lez, 54 a√±os. Fecha: 17/07/2025. M√©dico: Dr. P√©rez. S: Dolor de garganta..."
‚úÖ "Control de paciente diab√©tico. HbA1c 7.2%. Continuar metformina..."
‚úÖ "Sra. Ana Garc√≠a, 45 a√±os. Diagn√≥stico: Hipertensi√≥n. Prescribir enalapril..."

¬øEs una NOTA CL√çNICA para registrar (fhir) o una consulta normal?

Responde SOLO: fhir|normal
"""
                try:
                    response3 = await self.llm_classifier.ainvoke(clinical_note_prompt)
                    note_result = self._extract_response_text(response3).strip().lower()
                    
                    if "fhir" in note_result:
                        final_agent = "fhir"
                        logger.info(f"üß† Detectado como nota cl√≠nica: FHIR")
                except:
                    pass  # Mantener clasificaci√≥n anterior
            
            logger.info(f"üß† Clasificaci√≥n final: {final_agent}")
            
            return {
                'intent': f"Consulta clasificada como {final_agent}",
                'entities': {},
                'suggested_agent': final_agent,
                'justification': f"Clasificado por LLM como {final_agent}"
            }
                
        except Exception as e:
            logger.error(f"Error en clasificaci√≥n LLM: {e}")
            return {'intent': 'error', 'entities': {}, 'suggested_agent': 'greeting', 'justification': str(e)}

    async def _score_and_select_agent(self, analysis: Dict[str, Any], original_query: str) -> Tuple[str, float, str]:
        """
        SISTEMA DE FALLBACK INTELIGENTE CON PROMPTS ESPEC√çFICOS
        """
        suggested_agent = analysis.get('suggested_agent', 'greeting')
        justification = analysis.get('justification', 'Sin justificaci√≥n.')
        
        logger.info(f"[DEBUG] Agente sugerido: {suggested_agent}, agentes disponibles: {list(self.agents.keys())}")
        
        # Verificar si el agente sugerido est√° disponible
        if suggested_agent in self.agents:
            return suggested_agent, 0.9, justification
        else:
            logger.warning(f"Agente '{suggested_agent}' no disponible. Agentes disponibles: {list(self.agents.keys())}")
            
            # FALLBACK INTELIGENTE CON PROMPT ESPEC√çFICO
            if not self.llm_classifier:
                # Fallback simple sin LLM
                available_agents = [agent for agent in self.agents.keys() if agent != "greeting"]
                if available_agents:
                    fallback_agent = available_agents[0]
                    return fallback_agent, 0.5, f"Agente '{suggested_agent}' no disponible, usando {fallback_agent}"
                else:
                    return 'greeting', 0.3, "No hay agentes disponibles, usando greeting"
            
            # PROMPT DE FALLBACK INTELIGENTE
            fallback_prompt = f"""
CONSULTA ORIGINAL: "{original_query}"
AGENTE SUGERIDO (NO DISPONIBLE): {suggested_agent}
AGENTES DISPONIBLES: {list(self.agents.keys())}

Necesitas elegir el mejor agente alternativo disponible para manejar esta consulta.

EXPLICACI√ìN DE AGENTES DISPONIBLES:

1. **GREETING**: Para saludos y conversaci√≥n general
2. **CLINICAL_ANALYSIS**: Para explicaciones m√©dicas y medicamentos
3. **BIOCHAT**: Para b√∫squedas de investigaci√≥n cient√≠fica
4. **SQL**: Para consultas de base de datos de pacientes
5. **FHIR**: Para registrar informaci√≥n m√©dica

REGLAS DE FALLBACK:
- Si el agente sugerido era CLINICAL_ANALYSIS ‚Üí usar BIOCHAT si est√° disponible
- Si el agente sugerido era BIOCHAT ‚Üí usar CLINICAL_ANALYSIS si est√° disponible
- Si el agente sugerido era SQL ‚Üí usar FHIR si est√° disponible
- Si el agente sugerido era FHIR ‚Üí usar SQL si est√° disponible
- Si no hay alternativas espec√≠ficas ‚Üí usar el primer agente disponible que no sea GREETING

¬øCu√°l es el mejor agente alternativo para esta consulta?

Responde SOLO con el nombre del agente: greeting|clinical_analysis|biochat|sql|fhir
"""
            
            try:
                response = await self.llm_classifier.ainvoke(fallback_prompt)
                fallback_agent = self._extract_response_text(response).strip().lower().replace('.', '').replace('"', '').replace("'", "")
                
                # Validar que el agente de fallback est√© disponible
                if fallback_agent in self.agents:
                    logger.info(f"üß† Fallback inteligente: {suggested_agent} ‚Üí {fallback_agent}")
                    return fallback_agent, 0.7, f"Agente '{suggested_agent}' no disponible, LLM sugiri√≥ {fallback_agent}"
                else:
                    # √öltimo recurso: usar el primer agente disponible
                    available_agents = [agent for agent in self.agents.keys() if agent != "greeting"]
                    if available_agents:
                        final_fallback = available_agents[0]
                        return final_fallback, 0.5, f"Agente '{suggested_agent}' no disponible, usando {final_fallback}"
                    else:
                        return 'greeting', 0.3, "No hay agentes disponibles, usando greeting"
                        
            except Exception as e:
                logger.error(f"Error en fallback inteligente: {e}")
                # Fallback simple sin LLM
                available_agents = [agent for agent in self.agents.keys() if agent != "greeting"]
                if available_agents:
                    fallback_agent = available_agents[0]
                    return fallback_agent, 0.5, f"Agente '{suggested_agent}' no disponible, usando {fallback_agent}"
                else:
                    return 'greeting', 0.3, "No hay agentes disponibles, usando greeting"

    async def _smart_sql_vs_fhir_classification(self, query: str, intent: str, scores: Dict[str, float]):
        """
        ELIMINADA: Ya no se necesita esta funci√≥n compleja
        """
        pass

    def _get_user_memory_context(self) -> Dict[str, Any]:
        """Recupera el contexto de la memoria del usuario actual."""
        mem = self.user_memories.get(self.current_user_id)
        if mem and mem.last_patient_id:
            return {'last_patient_id': mem.last_patient_id}
        return {}
    
    def _extract_response_text(self, response: Any) -> str:
        """Extrae texto de la respuesta del LLM (puede ser string o un objeto)."""
        if isinstance(response, str):
            return response
        if hasattr(response, 'content'):
            return str(response.content)
        return str(response)

# Funci√≥n de utilidad para inicializaci√≥n r√°pida
async def create_flexible_orchestrator(config_path: Optional[str] = None) -> "IntelligentOrchestratorV3":
    """
    Crea una instancia del orquestador inteligente V3 con configuraci√≥n optimizada.
    """
    orchestrator = IntelligentOrchestratorV3(
        config_path=config_path,
        enable_cache=True,
        enable_performance_monitoring=True,
        cache_ttl_minutes=60
    )
    
    # Calentar el sistema con consultas comunes para asegurar que los modelos
    # y agentes est√©n listos.
    await orchestrator.process_query_optimized("Hola")
    await orchestrator.process_query_optimized("¬øCu√°ntos pacientes hay?")
    
    return orchestrator


# Funci√≥n principal para pruebas
async def main():
    """Funci√≥n principal para pruebas del orquestador v2.0"""
    orchestrator = await create_flexible_orchestrator()
    
    # Pruebas de rendimiento
    test_queries = [
        "Hola, ¬øc√≥mo est√°s?",
        "¬øCu√°ntos pacientes hay en la base de datos?",
        "Procesar nota: Paciente Juan P√©rez, 45 a√±os, hipertensi√≥n",
        "Buscar estudios sobre diabetes tipo 2",
        "Mostrar todos los procedimientos autorizados"
    ]
    
    print(f"{Colors.BOLD}üß™ PRUEBAS DE RENDIMIENTO{Colors.END}")
    
    for query in test_queries:
        start_time = time.time()
        result = await orchestrator.process_query_optimized(query)
        end_time = time.time()
        
        print(f"{Colors.GREEN}Consulta:{Colors.END} {query}")
        print(f"{Colors.BLUE}Tiempo:{Colors.END} {end_time - start_time:.3f}s")
        print(f"{Colors.CYAN}Resultado:{Colors.END} {str(result)[:100]}...")
        print("-" * 80)
    
    # Mostrar m√©tricas
    metrics = orchestrator.get_performance_metrics()
    print(f"{Colors.BOLD}üìä M√âTRICAS FINALES{Colors.END}")
    for key, value in metrics.items():
        print(f"{Colors.YELLOW}{key}:{Colors.END} {value}")


if __name__ == "__main__":
    asyncio.run(main()) 