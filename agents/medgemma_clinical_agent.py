#!/usr/bin/env python3
"""
MedGemma Clinical Agent - Agente de An√°lisis Cl√≠nico Avanzado
=============================================================

Este agente utiliza MedGemma para an√°lisis cl√≠nico especializado.
Basado en la documentaci√≥n oficial de MedGemma.

Autor: Carmen Pascual
Versi√≥n: 2.0
"""

import asyncio
import json
import logging
from typing import Dict, Any, List, Optional
import torch
from transformers.pipelines import pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM

logger = logging.getLogger(__name__)

# Funci√≥n auxiliar para llamar a LLM
def _call_openai_native(client, messages, temperature=0.1, max_tokens=4000, task_description="Consultando modelo de IA"):
    """Funci√≥n auxiliar para llamar a LLM de forma nativa"""
    try:
        if hasattr(client, 'invoke'):
            response = client.invoke(messages[0]["content"])
            return response
        elif hasattr(client, '__call__'):
            response = client(messages[0]["content"])
            return response
        else:
            return "Error: Cliente LLM no compatible"
    except Exception as e:
        return f"Error llamando LLM: {str(e)}"

class MockResponse:
    def __init__(self, content: str):
        self.content = content

class MedGemmaClinicalAgent:
    """
    Agente cl√≠nico especializado usando MedGemma para an√°lisis m√©dico avanzado.
    
    Caracter√≠sticas:
    - An√°lisis cl√≠nico de datos m√©dicos
    - Validaci√≥n de diagn√≥sticos
    - Explicaci√≥n de conceptos m√©dicos
    - Generaci√≥n de reportes cl√≠nicos
    - Recomendaciones de tratamiento
    """
    
    def __init__(self, model_id: str = "google/medgemma-27b-text-it", device: str = "auto", llm=None):
        """
        Inicializa el agente MedGemma con el modelo correcto.
        
        Args:
            model_id: ID del modelo MedGemma en Hugging Face
            device: Dispositivo a usar ("auto", "cuda", "cpu")
            llm: Cliente LLM para fallback din√°mico
        """
        self.model_id = model_id
        self.device = device
        self.llm = llm
        self.pipe = None
        self.model = None
        self.tokenizer = None
        self.is_initialized = False
        
        print("üè• Inicializando MedGemma Clinical Agent...")
        
        # Detectar dispositivo
        if torch.cuda.is_available():
            print("   ‚úÖ CUDA disponible - Usando GPU")
            self.device = "cuda"
        else:
            print("   ‚ö†Ô∏è CUDA no disponible - Usando CPU")
            self.device = "cpu"
        
        # Inicializar modelo
        self._initialize_model()
    
    def _initialize_model(self):
        """Inicializa el modelo MedGemma usando el pipeline de transformers"""
        try:
            print("   üîß Inicializando agente cl√≠nico...")
            logger.info("[DEBUG] Iniciando inicializaci√≥n del agente cl√≠nico...")
            
            # Intentar obtener token de Hugging Face si est√° disponible
            import os
            hf_token = os.getenv("HUGGINGFACE_TOKEN")
            
            # Intentar cargar MedGemma si est√° disponible
            try:
                if hf_token:
                    logger.info("[DEBUG] Intentando cargar MedGemma con token...")
                    self.pipe = pipeline(
                        "text-generation",
                        model="google/medgemma-27b-text-it",
                        token=hf_token,
                        torch_dtype=torch.bfloat16,
                        device_map="auto" if self.device == "cuda" else "cpu"
                    )
                    self.is_initialized = True
                    self.model_id = "google/medgemma-27b-text-it"
                    print("   ‚úÖ MedGemma inicializado correctamente")
                    logger.info("[DEBUG] MedGemma inicializado correctamente")
                    return
                else:
                    logger.warning("[DEBUG] No se encontr√≥ HUGGINGFACE_TOKEN, MedGemma no disponible")
            except Exception as e:
                logger.warning(f"[DEBUG] Error cargando MedGemma: {e}")
            
            # Si MedGemma no est√° disponible, usar fallback con LLM
            logger.info("[DEBUG] Usando fallback con LLM")
            print("   ‚ö†Ô∏è MedGemma no disponible, usando LLM como fallback")
            self.is_initialized = False
            self.pipe = self._create_fallback_pipeline()
            
        except Exception as e:
            print(f"   ‚ùå Error cr√≠tico inicializando agente: {e}")
            logger.error(f"[DEBUG] Error cr√≠tico inicializando agente: {e}")
            self.is_initialized = False
            self.pipe = self._create_fallback_pipeline()
            logger.info("[DEBUG] Usando pipeline de fallback por error cr√≠tico")
    
    def _create_fallback_pipeline(self):
        """Crea un pipeline de fallback din√°mico usando LLM"""
        logger.info("[DEBUG] Creando pipeline de fallback din√°mico...")
        
        class DynamicFallbackPipeline:
            def __init__(self, llm=None):
                self.llm = llm
                logger.info("[DEBUG] DynamicFallbackPipeline inicializado")
                
            async def __call__(self, text, **kwargs):
                logger.info(f"[DEBUG] DynamicFallbackPipeline llamado con texto: {text[:100]}...")
                
                if not self.llm:
                    return [{"generated_text": "Lo siento, no puedo proporcionar informaci√≥n m√©dica espec√≠fica en este momento. Por favor, consulte con un profesional de la salud."}]
                
                try:
                    # Prompt din√°mico para an√°lisis m√©dico
                    prompt = f"""Eres un asistente m√©dico experto. Analiza esta consulta y proporciona informaci√≥n m√©dica √∫til y precisa.

CONSULTA: "{text}"

TAREA: Proporciona informaci√≥n m√©dica relevante, segura y √∫til.

REGLAS:
- Si es sobre medicamentos, menciona efectos adversos comunes y graves
- Si es sobre enfermedades, explica s√≠ntomas y tratamientos generales
- Si es sobre diagn√≥sticos, proporciona informaci√≥n educativa
- SIEMPRE recomienda consultar con un profesional de la salud
- S√© preciso pero no hagas diagn√≥sticos espec√≠ficos
- Usa lenguaje m√©dico apropiado pero comprensible

RESPUESTA: Proporciona informaci√≥n m√©dica √∫til y estructurada."""

                    # Usar LLM para respuesta din√°mica
                    if self.llm:
                        try:
                            llm = self.llm  # Capturar referencia local
                            response = await asyncio.to_thread(
                                lambda: llm.invoke(prompt) if hasattr(llm, 'invoke') else str(llm(prompt))
                            )
                            content = str(response)
                        except Exception as e:
                            logger.error(f"[DEBUG] Error llamando LLM: {e}")
                            content = "Lo siento, no puedo proporcionar informaci√≥n m√©dica espec√≠fica en este momento. Por favor, consulte con un profesional de la salud."
                    else:
                        content = "Lo siento, no puedo proporcionar informaci√≥n m√©dica espec√≠fica en este momento. Por favor, consulte con un profesional de la salud."
                    
                    logger.info("[DEBUG] Respuesta din√°mica generada")
                    return [{"generated_text": content}]
                    
                except Exception as e:
                    logger.error(f"[DEBUG] Error en fallback din√°mico: {e}")
                    return [{"generated_text": "Lo siento, no puedo proporcionar informaci√≥n m√©dica espec√≠fica en este momento. Por favor, consulte con un profesional de la salud."}]
        
        return DynamicFallbackPipeline(self.llm)
    
    def _extract_response_text(self, response) -> str:
        """Extrae texto de respuesta del LLM"""
        if hasattr(response, 'content'):
            return str(response.content)
        elif isinstance(response, str):
            return response
        else:
            return str(response)
    
    def _create_medical_messages(self, prompt_type: str, **kwargs) -> List[Dict[str, str]]:
        """
        Crea mensajes m√©dicos estructurados para MedGemma.
        
        Args:
            prompt_type: Tipo de prompt ("analysis", "validation", "explanation", "report", "treatment")
            **kwargs: Datos espec√≠ficos para el prompt
            
        Returns:
            List[Dict[str, str]]: Mensajes estructurados
        """
        system_prompts = {
            "analysis": "You are an expert clinical analyst. Analyze the provided medical data and provide detailed clinical insights.",
            "validation": "You are a medical validation specialist. Validate the provided diagnosis against the symptoms and medical history.",
            "explanation": "You are a medical educator. Explain medical concepts in clear, understandable terms.",
            "report": "You are a clinical report generator. Create comprehensive medical reports based on the provided data.",
            "treatment": "You are a treatment recommendation specialist. Provide evidence-based treatment recommendations."
        }
        
        system_prompt = system_prompts.get(prompt_type, "You are a helpful medical assistant.")
        
        messages = [
            {
                "role": "system",
                "content": system_prompt
            }
        ]
        
        # Agregar contenido espec√≠fico seg√∫n el tipo
        if prompt_type == "analysis" and "medical_data" in kwargs:
            messages.append({
                "role": "user",
                "content": f"Please analyze this medical data and provide clinical insights:\n\n{kwargs['medical_data']}"
            })
        elif prompt_type == "validation" and "diagnosis" in kwargs and "symptoms" in kwargs:
            messages.append({
                "role": "user",
                "content": f"Validate this diagnosis: {kwargs['diagnosis']}\n\nSymptoms: {kwargs['symptoms']}\n\nMedical history: {kwargs.get('medical_history', 'Not provided')}"
            })
        elif prompt_type == "explanation" and "concept" in kwargs:
            messages.append({
                "role": "user",
                "content": f"Explain this medical concept: {kwargs['concept']}"
            })
        elif prompt_type == "report" and "patient_data" in kwargs and "medical_results" in kwargs:
            messages.append({
                "role": "user",
                "content": f"Generate a clinical report based on:\n\nPatient Data: {kwargs['patient_data']}\n\nMedical Results: {kwargs['medical_results']}"
            })
        elif prompt_type == "treatment" and "diagnosis" in kwargs and "medical_history" in kwargs:
            messages.append({
                "role": "user",
                "content": f"Provide treatment recommendations for:\n\nDiagnosis: {kwargs['diagnosis']}\n\nMedical History: {kwargs['medical_history']}"
            })
        
        return messages
    
    async def analyze_clinical_data(self, medical_data: str, stream_callback=None) -> Dict[str, Any]:
        """
        Analiza datos cl√≠nicos usando MedGemma.
        
        Args:
            medical_data: Datos m√©dicos a analizar
            stream_callback: Funci√≥n para mostrar progreso
            
        Returns:
            Dict[str, Any]: An√°lisis cl√≠nico
        """
        if not self.is_initialized:
            return {
                "success": False,
                "error": "MedGemma no est√° inicializado",
                "analysis": "An√°lisis no disponible - MedGemma no inicializado"
            }
        
        try:
            if stream_callback:
                stream_callback("   üß† Analizando datos cl√≠nicos con MedGemma...")
            
            # Crear mensajes para an√°lisis cl√≠nico
            messages = self._create_medical_messages("analysis", medical_data=medical_data)
            
            # Usar pipeline para generaci√≥n
            if not self.pipe:
                return {
                    "success": False,
                    "error": "Pipeline no inicializado",
                    "analysis": "An√°lisis no disponible - Pipeline no inicializado"
                }
            
            prompt_text = messages[-1]["content"] if messages else ""
            output = self.pipe(
                prompt_text, 
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7
            )
            
            # Extraer el contenido generado
            if output and isinstance(output, list) and len(output) > 0:
                generated_text = output[0]["generated_text"]
                response = str(generated_text)
                
                if stream_callback:
                    stream_callback("   ‚úÖ An√°lisis cl√≠nico completado")
                
                return {
                    "success": True,
                    "analysis": response,
                    "model": "MedGemma",
                    "tokens_generated": len(response.split())
                }
            else:
                return {
                    "success": False,
                    "error": "No se gener√≥ respuesta",
                    "analysis": "No se pudo generar an√°lisis cl√≠nico"
                }
                
        except Exception as e:
            logger.error(f"Error en an√°lisis cl√≠nico: {e}")
            return {
                "success": False,
                "error": str(e),
                "analysis": f"Error en an√°lisis: {str(e)}"
            }
    
    async def validate_diagnosis(self, diagnosis: str, symptoms: str, stream_callback=None) -> Dict[str, Any]:
        """
        Valida un diagn√≥stico usando MedGemma.
        
        Args:
            diagnosis: Diagn√≥stico a validar
            symptoms: S√≠ntomas del paciente
            stream_callback: Funci√≥n para mostrar progreso
            
        Returns:
            Dict[str, Any]: Validaci√≥n del diagn√≥stico
        """
        if not self.is_initialized:
            return {
                "success": False,
                "error": "MedGemma no est√° inicializado",
                "validation": "Validaci√≥n no disponible"
            }
        
        try:
            if stream_callback:
                stream_callback("   üîç Validando diagn√≥stico con MedGemma...")
            
            messages = self._create_medical_messages(
                "validation", 
                diagnosis=diagnosis, 
                symptoms=symptoms
            )
            
            if not self.pipe:
                return {
                    "success": False,
                    "error": "Pipeline no inicializado",
                    "validation": "Validaci√≥n no disponible"
                }
            
            prompt_text = messages[-1]["content"] if messages else ""
            output = self.pipe(
                prompt_text, 
                max_new_tokens=250,
                do_sample=True,
                temperature=0.6
            )
            
            if output and isinstance(output, list) and len(output) > 0:
                generated_text = output[0]["generated_text"]
                response = str(generated_text)
                
                if stream_callback:
                    stream_callback("   ‚úÖ Validaci√≥n completada")
                
                return {
                    "success": True,
                    "validation": response,
                    "diagnosis": diagnosis,
                    "symptoms": symptoms,
                    "model": "MedGemma"
                }
            else:
                return {
                    "success": False,
                    "error": "No se gener√≥ validaci√≥n",
                    "validation": "No se pudo validar el diagn√≥stico"
                }
                
        except Exception as e:
            logger.error(f"Error en validaci√≥n de diagn√≥stico: {e}")
            return {
                "success": False,
                "error": str(e),
                "validation": f"Error en validaci√≥n: {str(e)}"
            }
    
    async def explain_medical_concept(self, concept: str, stream_callback=None) -> Dict[str, Any]:
        """
        Explica un concepto m√©dico usando MedGemma.
        
        Args:
            concept: Concepto m√©dico a explicar
            stream_callback: Funci√≥n para mostrar progreso
            
        Returns:
            Dict[str, Any]: Explicaci√≥n del concepto
        """
        if not self.is_initialized:
            return {
                "success": False,
                "error": "MedGemma no est√° inicializado",
                "explanation": "Explicaci√≥n no disponible"
            }
        
        try:
            if stream_callback:
                stream_callback("   üìö Explicando concepto m√©dico con MedGemma...")
            
            messages = self._create_medical_messages("explanation", concept=concept)
            
            if not self.pipe:
                return {
                    "success": False,
                    "error": "Pipeline no inicializado",
                    "explanation": "Explicaci√≥n no disponible"
                }
            
            prompt_text = messages[-1]["content"] if messages else ""
            output = self.pipe(
                prompt_text, 
                max_new_tokens=200,
                do_sample=True,
                temperature=0.8
            )
            
            if output and isinstance(output, list) and len(output) > 0:
                generated_text = output[0]["generated_text"]
                response = str(generated_text)
                
                if stream_callback:
                    stream_callback("   ‚úÖ Explicaci√≥n completada")
                
                return {
                    "success": True,
                    "explanation": response,
                    "concept": concept,
                    "model": "MedGemma"
                }
            else:
                return {
                    "success": False,
                    "error": "No se gener√≥ explicaci√≥n",
                    "explanation": "No se pudo explicar el concepto"
                }
                
        except Exception as e:
            logger.error(f"Error explicando concepto m√©dico: {e}")
            return {
                "success": False,
                "error": str(e),
                "explanation": f"Error en explicaci√≥n: {str(e)}"
            }
    
    async def generate_clinical_report(self, patient_data: str, medical_results: str, stream_callback=None) -> Dict[str, Any]:
        """
        Genera un reporte cl√≠nico usando MedGemma.
        
        Args:
            patient_data: Datos del paciente
            medical_results: Resultados m√©dicos
            stream_callback: Funci√≥n para mostrar progreso
            
        Returns:
            Dict[str, Any]: Reporte cl√≠nico generado
        """
        if not self.is_initialized:
            return {
                "success": False,
                "error": "MedGemma no est√° inicializado",
                "report": "Reporte no disponible"
            }
        
        try:
            if stream_callback:
                stream_callback("   üìã Generando reporte cl√≠nico con MedGemma...")
            
            messages = self._create_medical_messages(
                "report", 
                patient_data=patient_data, 
                medical_results=medical_results
            )
            
            if not self.pipe:
                return {
                    "success": False,
                    "error": "Pipeline no inicializado",
                    "report": "Reporte no disponible"
                }
            
            prompt_text = messages[-1]["content"] if messages else ""
            output = self.pipe(
                prompt_text, 
                max_new_tokens=400,
                do_sample=True,
                temperature=0.7
            )
            
            if output and isinstance(output, list) and len(output) > 0:
                generated_text = output[0]["generated_text"]
                response = str(generated_text)
                
                if stream_callback:
                    stream_callback("   ‚úÖ Reporte cl√≠nico generado")
                
                return {
                    "success": True,
                    "report": response,
                    "patient_data": patient_data,
                    "medical_results": medical_results,
                    "model": "MedGemma"
                }
            else:
                return {
                    "success": False,
                    "error": "No se gener√≥ reporte",
                    "report": "No se pudo generar el reporte cl√≠nico"
                }
                
        except Exception as e:
            logger.error(f"Error generando reporte cl√≠nico: {e}")
            return {
                "success": False,
                "error": str(e),
                "report": f"Error generando reporte: {str(e)}"
            }
    
    async def recommend_treatment(self, diagnosis: str, medical_history: str, stream_callback=None) -> Dict[str, Any]:
        """
        Recomienda tratamiento usando MedGemma.
        
        Args:
            diagnosis: Diagn√≥stico del paciente
            medical_history: Historia m√©dica
            stream_callback: Funci√≥n para mostrar progreso
            
        Returns:
            Dict[str, Any]: Recomendaciones de tratamiento
        """
        if not self.is_initialized:
            return {
                "success": False,
                "error": "MedGemma no est√° inicializado",
                "recommendations": "Recomendaciones no disponibles"
            }
        
        try:
            if stream_callback:
                stream_callback("   üíä Generando recomendaciones de tratamiento con MedGemma...")
            
            messages = self._create_medical_messages(
                "treatment", 
                diagnosis=diagnosis, 
                medical_history=medical_history
            )
            
            if not self.pipe:
                return {
                    "success": False,
                    "error": "Pipeline no inicializado",
                    "recommendations": "Recomendaciones no disponibles"
                }
            
            prompt_text = messages[-1]["content"] if messages else ""
            output = self.pipe(
                prompt_text, 
                max_new_tokens=350,
                do_sample=True,
                temperature=0.6
            )
            
            if output and isinstance(output, list) and len(output) > 0:
                generated_text = output[0]["generated_text"]
                response = str(generated_text)
                
                if stream_callback:
                    stream_callback("   ‚úÖ Recomendaciones generadas")
                
                return {
                    "success": True,
                    "recommendations": response,
                    "diagnosis": diagnosis,
                    "medical_history": medical_history,
                    "model": "MedGemma"
                }
            else:
                return {
                    "success": False,
                    "error": "No se generaron recomendaciones",
                    "recommendations": "No se pudieron generar recomendaciones"
                }
                
        except Exception as e:
            logger.error(f"Error generando recomendaciones: {e}")
            return {
                "success": False,
                "error": str(e),
                "recommendations": f"Error generando recomendaciones: {str(e)}"
            }
    
    async def process_query(self, query: str, stream_callback=None) -> Dict[str, Any]:
        """
        Procesa consultas m√©dicas con an√°lisis cl√≠nico avanzado y streaming en tiempo real.
        """
        try:
            if not self.llm:
                return {
                    "success": False,
                    "message": "‚ùå LLM no disponible para an√°lisis cl√≠nico",
                    "model": "Sin LLM"
                }
            
            # An√°lisis de la consulta m√©dica
            if stream_callback:
                stream_callback("üß† Analizando consulta m√©dica...")
            
            # Generar respuesta con streaming
            response_text = await self._generate_streaming_response(query, stream_callback)
            
            return {
                "success": True,
                "response": response_text,
                "query": query,
                "model": "LLM M√©dico Especializado"
            }
            
        except Exception as e:
            logger.error(f"Error en an√°lisis cl√≠nico: {e}")
            return {
                "success": False,
                "message": f"‚ùå Error en an√°lisis cl√≠nico: {str(e)}",
                "model": "Error"
            }

    async def _generate_streaming_response(self, query: str, stream_callback=None) -> str:
        """
        Genera respuesta con streaming en tiempo real.
        """
        try:
            if not self.llm:
                return "‚ùå LLM no disponible para an√°lisis cl√≠nico"
            
            # Prompt para an√°lisis cl√≠nico
            prompt = f"""Eres un m√©dico experto que proporciona informaci√≥n cl√≠nica precisa y √∫til.

CONSULTA DEL PACIENTE: "{query}"

INSTRUCCIONES:
1. Proporciona informaci√≥n m√©dica clara y precisa
2. Incluye dosis recomendadas cuando sea apropiado
3. Menciona efectos secundarios importantes
4. Advierte sobre cu√°ndo consultar al m√©dico
5. Usa lenguaje comprensible para el paciente
6. S√© espec√≠fico y pr√°ctico en las recomendaciones

FORMATO DE RESPUESTA:
- Informaci√≥n clara y estructurada
- Dosis y frecuencia cuando aplique
- Efectos secundarios importantes
- Cu√°ndo buscar atenci√≥n m√©dica
- Precauciones alimentarias si aplica

Responde de manera profesional y √∫til:"""

            if stream_callback:
                stream_callback("ü§ñ Generando respuesta m√©dica...")
            
            # Usar streaming real del LLM
            response = await self.llm.ainvoke(prompt)
            
            # Extraer el texto de la respuesta
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            if stream_callback:
                stream_callback("‚úÖ An√°lisis cl√≠nico completado")
            
            return response_text.strip()
            
        except Exception as e:
            logger.error(f"Error generando respuesta con streaming: {e}")
            return f"‚ùå Error generando respuesta: {str(e)}"

async def test_medgemma_agent():
    """Funci√≥n de prueba para el agente MedGemma"""
    print("üß™ Probando agente MedGemma...")
    
    agent = MedGemmaClinicalAgent()
    
    if agent.is_initialized:
        print("‚úÖ MedGemma inicializado correctamente")
        
        # Prueba de an√°lisis cl√≠nico
        test_data = """
        Paciente masculino, 52 a√±os
        Diagn√≥stico: Hipertensi√≥n arterial
        Presi√≥n arterial: 150/95 mmHg
        Medicaci√≥n: Enalapril 10mg/d√≠a
        """
        
        result = await agent.analyze_clinical_data(test_data)
        print(f"üìä Resultado del an√°lisis: {result}")
        
    else:
        print("‚ùå MedGemma no se pudo inicializar")

if __name__ == "__main__":
    asyncio.run(test_medgemma_agent()) 